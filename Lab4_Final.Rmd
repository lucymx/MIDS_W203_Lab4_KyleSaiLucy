---
title: "Lab 4"
author: "Kyle Redfield, Sai Ruvuru, Lucy Xie"
date: "December 12, 2017"
output: pdf_document
---

# Introduction
The objective of this regression analysis is to explore the determinants of crime based on the provided dataset and support policy suggestions for local government. Three regression models were created and checked against six standard assumptions: linear parameters, random sampling, zero-conditional mean, normality of standard errors, homoskedastiity, and no perfect multicollinearity. The first model includes only explanatory variables of key interest; the second model adds in unbiased covariate inputs based on our prior understanding of crime; and the third model includes all of the available inputs to demonstrate the robustness of using the entire dataset compared to rigorous selection.

The process of selecting model inputs, checking assumptions, and interpreting the coefficients' statistical and practical significance is documented below.

```{r, warning=FALSE}
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
library(datasets)

#Load dataset
#setwd('C:\\Users\\Lucy\\Documents\\Berkeley MIDS\\W203 Statistics for Data Science\\Lab 4')
data = read.csv("crime.csv")

```

# Exploratory Data Analysis (EDA)

An exploratory analysis of the data is conducted to identify anomalies and potential transformations. 

## Initial EDA and Data Preprocessing
The size of the dataset is large but relatively small at `r nrow(data)`. The variables to begin the EDA is as following:
```{r}
str(data)
```

A summary and snapshot of the data is taken to examine for anomalies:
```{r}
# Examine size and shape of data
summary(data)
head(data)
```

While no missing values(NAs) are identified, the summary table shows that some of the inputs expressed as probabilities have values over 100%, which is impossible. The data is subset and excluded of any rows where "prbarr", "prbconv" or "prbpris" is greater than 1. The size of the dataset decreases from 90 to 80 rows.

```{r}
# Filter out values of prbarr, prbconv and prbpris >1 and count 
# the remaining rows
data_sub <- data[which((data$prbarr <= 1) & (data$prbconv <= 1) & (data$prbpris <= 1)), ]
nrow(data_sub)
head(data_sub)
```

The dependent variable, crime rate(`crmrte`) is further explored. 

```{r}
# Examine the dependent variable crmrte. 
hist(data_sub$crmrte)
qqnorm(data_sub$crmrte)
```
The variable is positively skewed as further supported by the qqnorm plot. Since the expectation of the population model from background research supports the skewed distribution of crimes committed per person at a low mean(`r mean(data_sub$crmrte)`) in relationship with the other normalized variables on the order of per capita, probability and per square mile, a log-log transformation will not be utilized in models.

## Input Selection

Focusing on the potential independent variables, a preliminary correlation matrix is created to check for potential multicollinearity between inputs, as well as to identify inputs with the highest correlation to crmrte. A univariate model is also created for every input variable and compared the BIC scores.

```{r}
# Create correlation matrix
round(cor(data[c(-1:-3)]),2)

# Print BIC score for linear model between each input and crime rate
n = 1
for (i in data) {
  #if (is.numeric(i[1])) {
    (model1 = lm(crmrte ~ i, data=data))
     print(colnames(data)[n])
     print(BIC(model1))
     last_BIC <- BIC(model1)
  #}
  n = n + 1
}
```

Both methods show that population density, urban indicator and federal wage have the highest individual influence on crime rate. In addition, examined police per capita is explored as a potential covariate to the model.

```{r, warning=FALSE}
# Examine population density, urban indicator, federal wage and police per capita
scatterplotMatrix(data_sub[ , c("density","urban","wfed","polpc")])

```
The positively skewed distributions of population density and police per capita indicates that lognormal transformation of the variables is needed.

# Model 1

As identified in the EDA, the top three variables with the highest influence on crime rate were population density, urban indicator and federal wage.  The urban indicator from the first model will be eliminated due to its collinearity with population density; by definition, regions classified as a SMSA have high population densities. The federal wage is also excluded due to lack of practical signifiance in relation to crime rate. Therefore, the first model examines the relationship between crime rate and population density, which is first transformed.

```{r}
# Create model of crime rate based on log(density)
(model1 = lm(crmrte ~ log(density), data=data_sub))

```

## Checking Assumptions
The model is first evaluated under whether all OLS assumptions are met.

First, the model meets the assumption of a linear relationship since the model is a linear combination of variables.

Second, the data in the model is from a dataset about little is known. From references in the codebook, the data is exclusively from North Carolina, suggesting that any extrapolation of the population model from this data cannot be accurately performed. However, there is no evidence that the data was not collected randomly from within North Carolina. In fact, if the data is exclusively from North Carolinian counties, it represents 90% of the counties in North Carolina as of 1987. As long as the remaining 10 counties were excluded in a non-systemic fashion, we can assume the data is randomly collected but only from within North Carolina.

Third, because there is only one input variable to this model, there is no risk of multicollinearity between inputs. This is mitigated by eliminating the urban indicator as a model input.

```{r}
# Check for zero-conditional mean
# Check residuals vs. fitted plot
plot(model1, which=1)
```

From the residuals vs. fitted values plot above, is can be seen that the values for residuals decrease, then increase along the fitted values axis. This normally is a sign that the zero-conditional mean assumption is violated; however, the tail ends of the fitted values have very few data points and have high influence on the "U" shape of the curve. Overall, the residuals are near zero and we accept the zero-conditional mean assumption.

```{r}
# Check for normality of errors
# Visualize distribution of residuals
hist(model1$residuals)

# Conduct Shapiro Test
shapiro.test(model1$residuals)

# Check normal Q-Q plot
plot(model1, which=2)
```

Both the histogram of residuals and Shapiro test suggest that the residuals are not normally distributed. The null hypothesis of the Shapiro states that the array is distributed normally. The p value is less than .01, suggesting that the null hypothesis is to be rejected. Finally, the residuals deviate from the diagonal line at lower and higher theoretical quantities, which further indicates non-normality.

[HOW DO WE MITIGATE? CAN RELY ON SAMPLE SIZE > 30?]

```{r}
# Check for homoskedasticity
# Check standardized residual plot
plot(model1, which=3)

# Conduct Breusch-Pagan Test
bptest(model1)

```
The above plot of residuals vs. fitted values shows a slight increase in the variance of residuals as fitted values increase. The standardized residuals plot confirms the increasing trend, but note that it decreases slightly at the high end of the fitted values axis, where there are much fewer data points. Therefore, a more robust method such as the Breusch-Pagan test must be used. The null hypothesis for the BP test is homoskedasticity. With this p-value, we fail to reject the null; therefore, homoskedasticity can be assumed.

## Statistical and Practical Significance
```{r}
summary(model1)
```

The significance codes in the output summary show that both the log(density) variable and the y-intercept are significant towards the resulting crime rate. 

```{r}
# Calculate standard deviation and mean of each variable
sd(data_sub$density)
sd(data_sub$crmrte)
mean(data_sub$density)
mean(data_sub$crmrte)
```
The coefficient on log-transformed population density suggests an increase of population density by one percentage point is associated with an increase in the crime rate of 0.00016. Because a normal standard deviation of density is about 1.5 percentage points, we could expect a change in one standard deviation of population density to be associated with an increase in crime of about .00016, or about one-tenth of a standard deviation of crime rate. This could be considered an economically significant effect. The y-intercept is also practically significant; as the density goes towards zero, the crime rate approaches 0.034, which is only slightly below the mean crime rate of 0.0355.

# Model 2
Because population density is likely not the only factor that influences the crime rate of an area, additional variables are included in the specification of Model 2, listed above as the effect of population density (transformed), police per capita (transformed), the probability of arrest, and the percent of minorities in the county in 1980 on the crime rate.

These variables are chosen since:
  - Police per capita should be closely related to crime since it is the primary deterrant of crime
  - The probability of arrest is a proxy for how averse people are to committing crime in that community. The crime rate should decrease as people's aversion to arrest increases.
  - Previous studies have drawn a link between the presence of minority populations and the crime rate. The inclusion of this factor is consistent with those studies.

```{r}
(model2 = lm(crmrte ~ log(density) + prbarr + pctmin80 + log(polpc), data = data_sub))
```

## Checking Assumptions
In the tests below, we see no major deviations from the diagnostics we saw in Model 1, with the exception of heteroskedasticity:

```{r}
# Zero-conditional mean
plot(model2, which=1)

# No perfect multicollinearity 
vif(model2)

# Normality of errors 
hist(model2$residuals)
plot(model2, which=2)
shapiro.test(model2$residuals)

# Homoskedasticity
plot(model2, which=3)
bptest(model2)

```
The p-value resulting from the Breusch-Pagan test rejects the null hypothesis of homoskedasticity. As a result, we must use robust standard errors.

```{r}
coeftest(model2, vcov = vcovHC)
vcovHC(model2)

```
With the exception of homoskedasticity, all of the confirmed and unconfirmed assumptions from the previous model hold into this model as well. There are some stronger signs of conditional mean on the residuals vs. fitted values plot, but this is again heavily influenced by the sparase data points on either end of the fitted values axis.

## Statistical and Practical Significance
```{r}
summary(model2)
```

It can be seen that all of these variables, including the intercept, have a statistically significant effect on the crime rate. Further the adjusted Rsquared has increased to .7572 from the value of .4695 in model 1. As a result, it can be determined that these variables increase the predictive power of the model over the decreases in parsmiony. 

# Model 3
Finally, a model is specified with all variables in the dataset included. Note we continue to transform the density and polpc variables. 

```{r}
(model3 = lm(crmrte ~ county + log(density) + prbarr + prbconv + prbpris + avgsen + log(polpc) + taxpc + west + central + urban + pctmin80 + wcon + wtuc +wtrd + wfir +wser + wmfg + wfed + wsta + wloc + mix + pctymle, data = data_sub))
```

## Checking Assumptions
```{r, warning=FALSE}
# Zero-conditional mean
plot(model3, which=1)

# No perfect multicollinearity
vif(model3)

# Normality of errors
hist(model3$residuals)
plot(model3, which=2)
shapiro.test(model3$residuals)

# Homoskedasticity
plot(model3, which=3)
bptest(model3)
```

One difference in this model is that errors are now normally distributed; we fail to reject the hypothesis of normality given in the Shapiro test. However, many of the other assumptions are violated. The residuals are not near 0, there are several points with high residuals and leverage, some variables have high degrees of collinearity, and heteroskedacity is again introduced.

## Statistical and Practical Signifiance
```{r}
summary(model3)
```
The significance codes show that several of the inputs used in Model 3 are not statistically significant. The most significant inputs were density, probablity of arrest, police per capita, and service industry wage, most of which were captured in Model 2 based on our prior understanding of crime.

[COMMENT ON PRACTICAL SIGNIFICANCE]

The results for all three models are compiled in the following, applying robust standard errors to Models 2 and 3:

```{r, warning=FALSE}
# Apply robust standard errors
se.model2 = sqrt(diag(vcovHC(model2)))
se.model3 = sqrt(diag(vcovHC(model3)))

# Print formatted regression table
stargazer(model1, model2, model3, type = "text", omit.stat = "f", se = list(se.model2,se.model3), star.cutoffs = c(0.05, 0.01, 0.001 ))
```

### Causality in the Model

The three models we specify above range in their ability to be interpreted causally. The first model has only one variable. Though it meets the assumptions required to be considered unbiased, it almost certainly has omitted variable bias. Model 2 inclues more variables and still meets the assumptions. There is likely less ommitted variable bias and more ability to discuss causality in Model 2. Model 3 should have the least amount of ommitted variable bias, since it incorporates all of the provided data; however, Model 3 shows multiple instances of multicollinearity and does not meet the assumptions required for unbiasedness or consistency. Therefore, while we can perhaps observe the direction of its coefficients, we are unable to draw any conclusions about causality from it.

### Conclusion
Of the three models generated, Model 2 shows the greatest balance between fit and parsimony while upholding the six assumptions for multiple regression. While Model 1 has no risk of multicollinearity, it has the lowest adjusted R-squared value and does not capture the behavior of the data well. Meanwhile, Model 3 has the highest adjusted R-squared value but introduces violations to several of the assumptions, like multicollinearity and conditional residual mean. Therefore, we can move forward with Model 2 to recommend policy changes to our local government. The population density, probability of arrest, percent of minorities and police per capita all have a large influence on the crime rate in that region. Police per capita is an interesting input, as it can also be interpreted as the result of high crime rate instead of the converse. The most surprising outcome is the inverse relationship between probability of arrest and crime rate. This is unintuitive, but can be rationalized; perhaps there are many crimes committed where the culprit is not identified, or where the police force is more lenient on arrests for small crimes. We recommend that government audits areas where the crime rate is high but probability of arrest is low. The method for calculating the probability of arrest also needs to be further explored.

